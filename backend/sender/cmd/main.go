package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"time"

	"github.com/confluentinc/confluent-kafka-go/v2/kafka"
	"github.com/jackc/pgx/v5"
	"github.com/joho/godotenv"
)

type SongAction struct {
	Action  string `json:"action"`
	UserID  string `json:"user_id"`
	TrackID string `json:"track_id"`
}

func main() {
	// 0) –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å .env
	envPath := filepath.Join("..", ".env")
	if err := godotenv.Load(envPath); err != nil {
		log.Printf("‚ö† warning: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å %s: %v", envPath, err)
	} else {
		log.Printf("‚úÖ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª %s", envPath)
	}

	// 1) –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Postgres (rest –æ–ø—É—â–µ–Ω, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)...
	dbHost := os.Getenv("DB_HOST")
	dbPort := os.Getenv("DB_PORT")
	dbUser := os.Getenv("DB_USER")
	if dbUser == "" {
		dbUser = os.Getenv("POSTGRES_USER")
	}
	dbPass := os.Getenv("DB_PASSWORD")
	if dbPass == "" {
		dbPass = os.Getenv("POSTGRES_PASSWORD")
	}
	dbName := os.Getenv("DB_NAME")
	if dbName == "" {
		dbName = os.Getenv("POSTGRES_DB")
	}
	dbURL := fmt.Sprintf("postgres://%s:%s@%s:%s/%s?sslmode=disable", dbUser, dbPass, dbHost, dbPort, dbName)

	var db *pgx.Conn
	var err error
	for attempt := 1; attempt <= 5; attempt++ {
		db, err = pgx.Connect(context.Background(), dbURL)
		if err == nil {
			break
		}
		log.Printf("‚ö† –ø–æ–ø—ã—Ç–∫–∞ %d: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è: %v", attempt, err)
		time.Sleep(time.Duration(attempt*2) * time.Second)
	}
	if err != nil {
		log.Fatalf("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î: %v", err)
	}
	defer db.Close(context.Background())
	log.Println("‚úÖ Connected to Postgres:", dbURL)

	// 2) Producer Kafka
	kafkaBrokers := os.Getenv("KAFKA_BOOTSTRAP_SERVERS")
	p, err := kafka.NewProducer(&kafka.ConfigMap{"bootstrap.servers": kafkaBrokers})
	if err != nil {
		log.Fatalf("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ–¥—å—é—Å–µ—Ä: %v", err)
	}
	defer p.Close()
	log.Println("‚úÖ Kafka producer, brokers:", kafkaBrokers)

	// –°–æ–±—ã—Ç–∏—è –¥–æ—Å—Ç–∞–≤–∫–∏
	go func() {
		for e := range p.Events() {
			if m, ok := e.(*kafka.Message); ok {
				if m.TopicPartition.Error != nil {
					log.Printf("‚ùå Delivery failed: %v", m.TopicPartition)
				} else {
					log.Printf("‚úÖ Delivered to %v", m.TopicPartition)
				}
			}
		}
	}()

	log.Println("üîÑ –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ deferred_tasks")
	for {
		tx, err := db.Begin(context.Background())
		if err != nil {
			log.Fatalf("Begin tx error: %v", err)
		}

		rows, err := tx.Query(context.Background(),
			`SELECT id, topic, data FROM deffered_tasks ORDER BY created_at ASC FOR UPDATE SKIP LOCKED LIMIT 20`)
		if err != nil {
			tx.Rollback(context.Background())
			log.Fatalf("Select tasks error: %v", err)
		}

		type taskRow struct {
			id, topic, data string
		}
		var tasks []taskRow
		for rows.Next() {
			var tr taskRow
			if err := rows.Scan(&tr.id, &tr.topic, &tr.data); err != nil {
				log.Printf("Scan error: %v", err)
				continue
			}
			tasks = append(tasks, tr)
		}
		rows.Close()

		if len(tasks) == 0 {
			tx.Rollback(context.Background())
			time.Sleep(5 * time.Second)
			continue
		}

		for _, tr := range tasks {
			// ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî –ù–û–í–û–ï: –ª–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å JSON –∏ —Ä–∞—Å–ø–∞—Ä—à–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
			log.Printf("üîî –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞–¥–∞—á–∞: id=%s, topic=%s, raw data=%s", tr.id, tr.topic, tr.data)
			var action SongAction
			if err := json.Unmarshal([]byte(tr.data), &action); err != nil {
				log.Printf("‚ö† –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å data –¥–ª—è –∑–∞–¥–∞—á–∏ %s: %v", tr.id, err)
			} else {
				log.Printf("‚îî‚îÄ parsed SongAction: action=%s, user_id=%s, track_id=%s",
					action.Action, action.UserID, action.TrackID)
			}
			// ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

			// Produce –≤ Kafka —Å retry
			var prodErr error
			for attempt := 1; attempt <= 3; attempt++ {
				prodErr = p.Produce(&kafka.Message{
					TopicPartition: kafka.TopicPartition{Topic: &tr.topic, Partition: kafka.PartitionAny},
					Key:            []byte(tr.id),
					Value:          []byte(tr.data),
				}, nil)
				if prodErr == nil {
					break
				}
				log.Printf("‚ö† Produce attempt %d for %s failed: %v", attempt, tr.id, prodErr)
				time.Sleep(time.Duration(attempt) * 500 * time.Millisecond)
			}
			if prodErr != nil {
				log.Fatalf("‚ùå Failed to produce %s: %v", tr.id, prodErr)
			}

			// –£–¥–∞–ª—è–µ–º –∑–∞–¥–∞—á—É
			if _, err := tx.Exec(context.Background(), `DELETE FROM deffered_tasks WHERE id = $1`, tr.id); err != nil {
				tx.Rollback(context.Background())
				log.Fatalf("Failed to delete task %s: %v", tr.id, err)
			}
			log.Printf("‚úÖ –ó–∞–¥–∞—á–∞ %s —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –∏ —É–¥–∞–ª–µ–Ω–∞", tr.id)
		}

		if err := tx.Commit(context.Background()); err != nil {
			log.Fatalf("Tx commit error: %v", err)
		}
	}
}
